{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python standard library\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Scipy\n",
    "import numpy as np\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import sklearn.utils\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter\n",
    "import IPython.display\n",
    "\n",
    "# Program-specific\n",
    "sys.path.append('../modules')\n",
    "import constants\n",
    "import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of rows in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = constants.base_file_path + '/data/train/train_all_filled.csv'\n",
    "test_file_path = constants.base_file_path + '/data/test/raw/test_reformatted.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12832\n"
     ]
    }
   ],
   "source": [
    "with open(train_file_path, 'r') as train_file_handle:\n",
    "    N_train_all = len(train_file_handle.readlines()) - 1\n",
    "print(N_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Folds cross-validation for C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Folds parameters\n",
    "K = 10\n",
    "kfolds = sklearn.model_selection.KFold(n_splits = K)\n",
    "Cs = [1e-5*(1.5**i) for i in range(7)]\n",
    "errors = [0 for i in range(len(Cs))]\n",
    "\n",
    "print(Cs)\n",
    "\n",
    "\n",
    "# Mini-batch parameters\n",
    "num_passes = 5\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "\n",
    "# Hyper parameter loop\n",
    "for i, C in enumerate(Cs):\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Benchmarking\n",
    "    '''\n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    t = time.time()\n",
    "    print('i = ', str(i), '/', str(len(Cs)), '\\t\\tC = ', C)\n",
    "    print('dt = ', str((t - t0)/60.))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # K-folds loop\n",
    "    for j, (train_indices, valid_indices) in enumerate(kfolds.split(np.empty((N_train_all)))):\n",
    "        print('\\tj = ', str(j), '/', K)\n",
    "    \n",
    "    \n",
    "        # Create the model trained on this fold\n",
    "        model = sklearn.linear_model.SGDClassifier(loss= 'log', alpha = 1./C)\n",
    "    \n",
    "                                                     \n",
    "                                                     \n",
    "        # Mini-batch loop\n",
    "        for k in range(num_passes):\n",
    "            print('\\t\\tk = ', str(k), '/', num_passes)\n",
    "                                                     \n",
    "            np.random.shuffle(train_indices)\n",
    "            \n",
    "            for l in range(int(len(train_indices)/batch_size + 1)):\n",
    "                print('\\t\\t\\tl = ', str(l), '/', str(int(len(train_indices)/batch_size + 1)))\n",
    "\n",
    "                # Get batch indices\n",
    "                if l == int(len(train_indices)/batch_size+1):\n",
    "                    # Last batch\n",
    "                    batch_train_indices = train_indices[l*batch_size:]\n",
    "                else:\n",
    "                    # Not last batch\n",
    "                    batch_train_indices = train_indices[l*batch_size:(l+1)*batch_size]\n",
    "                                                     \n",
    "                                \n",
    "                                                     \n",
    "                # Load in the training data for this particular batch\n",
    "                df_train = pd.read_csv(train_file_path, skiprows = [row + 1 for row in range(N_train_all) if row not in batch_train_indices], sep = ',', header = 0)\n",
    "\n",
    "\n",
    "                # Fit model\n",
    "                model.partial_fit(df_train[constants.inputs], df_train[constants.output], classes = [0, 1])\n",
    "                                                     \n",
    "                                                     \n",
    "                                                     \n",
    "                                                     \n",
    "                        \n",
    "                \n",
    "                                                     \n",
    "        # Load the validation data\n",
    "        df_valid = pd.read_csv(train_file_path, skiprows = [row + 1 for row in range(N_train_all) if row not in valid_indices], sep = ',', header = 0)\n",
    "                                                                       \n",
    "        \n",
    "        # Test model\n",
    "        predictions = model.predict_proba(df_valid[constants.inputs])\n",
    "\n",
    "        # Get errors\n",
    "        errors[i] += functions.LogLoss(predictions, df_valid[constants.output])/K\n",
    "        \n",
    "\n",
    "    print(Cs[i], errors[i])\n",
    "                                                     \n",
    "\n",
    "                                                     \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input in constants.inputs:\n",
    "    print(input)\n",
    "    if input not in df_train.columns.values:\n",
    "        print('asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train[constants.inputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(Cs, errors, marker = 'o')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('Log loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(Cs)):\n",
    "    print(Cs[i], '\\t\\t', errors[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = functions.LoadTrainData(aug = False, mix = True)\n",
    "N_train_all = len(df_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.loc[df_train['inc_angle'] == 'na', 'inc_angle'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_passes = 20\n",
    "batch_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "C = 7.6e-5\n",
    "\n",
    "\n",
    "\n",
    "train_indices = [i for i in range(N_train_all)]\n",
    "\n",
    "\n",
    "# Create the model trained on this fold\n",
    "model = sklearn.linear_model.SGDClassifier(loss= 'log', alpha = 1./C, n_jobs = -1)\n",
    "\n",
    "\n",
    "# Mini-batch loop\n",
    "for i in range(num_passes):\n",
    "    print('i = ', str(i), '/', num_passes)\n",
    "\n",
    "    np.random.shuffle(train_indices)\n",
    "\n",
    "    for j in range(int(len(train_indices)/batch_size + 1)):\n",
    "        print('\\tj = ', str(j), '/', str(int(len(train_indices)/batch_size + 1)))\n",
    "\n",
    "        # Get batch indices\n",
    "        if j == int(len(train_indices)/batch_size+1):\n",
    "            # Last batch\n",
    "            batch_train_indices = train_indices[j*batch_size:]\n",
    "        else:\n",
    "            # Not last batch\n",
    "            batch_train_indices = train_indices[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "\n",
    "\n",
    "        # Load in the training data for this particular batch\n",
    "        #df_train = pd.read_csv(train_file_path, skiprows = [row + 1 for row in range(N_train_all) if row not in batch_train_indices], sep = ',', header = 0)\n",
    "\n",
    "        # Fit model\n",
    "        model.partial_fit(df_train[constants.inputs].iloc[train_indices], df_train[constants.output].iloc[train_indices], classes = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_file_path, header = 0, sep = ',', index_col = 'id')\n",
    "\n",
    "predictions = model.predict_proba(df_test[constants.inputs])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[constants.inputs].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[constants.inputs].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file_path = '../data/submissions/submission_' + str(datetime.datetime.now().date()) + '_' + str(datetime.datetime.now().time()).replace(':', '-').split('.')[0]\n",
    "\n",
    "with open(output_file_path, 'w') as output_file_handle:\n",
    "    file_writer = csv.writer(output_file_handle, delimiter = ',')\n",
    "    \n",
    "    file_writer.writerow(['id', 'is_iceberg'])\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        file_writer.writerow([df_test.index.values[i], predictions[i]])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
